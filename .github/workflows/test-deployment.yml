name: Helm Chart Quality Test

# This workflow tests the ROS-OCP Helm chart deployment on Kind using the local chart
# Deployment flow:
#   1. Setup KIND cluster
#   2. Deploy Kafka infrastructure (Strimzi + Kafka) - prerequisite
#   3. Deploy ROS-OCP application
#
# Uses alternative registries to avoid Docker Hub rate limiting:
# - public.ecr.aws for official images (postgres, redis, busybox, confluent)
# - quay.io for application images (already configured)
# - ghcr.io as fallback for some images
# 
# Chart Source: Uses local ros-ocp/ directory instead of GitHub releases

on:
  pull_request:
    paths:
      - 'ros-ocp/**'
      - 'scripts/install-helm-chart.sh'
      - 'scripts/deploy-strimzi.sh'
      - 'scripts/deploy-kind.sh'

  workflow_dispatch:

jobs:
  helm-chart-test:
    name: Test Helm Chart Deployment
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      CONTAINER_RUNTIME: podman

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Podman
        run: |
          # Install Podman
          sudo apt-get update
          sudo apt-get install -y podman

          # Verify Podman installation
          echo "Podman version:"
          podman --version
          echo "Podman info:"
          podman info
          echo "Testing Podman connectivity:"
          podman run --rm quay.io/podman/hello

      - name: Install KIND
        run: |
          # Get the latest KIND version
          KIND_VERSION=$(curl -s https://api.github.com/repos/kubernetes-sigs/kind/releases/latest | grep '"tag_name":' | sed -E 's/.*"([^"]+)".*/\1/')
          echo "Latest KIND version: $KIND_VERSION"

          # Download and install the latest KIND version
          curl -Lo ./kind "https://kind.sigs.k8s.io/dl/${KIND_VERSION}/kind-linux-amd64"
          chmod +x ./kind
          sudo mv ./kind /usr/local/bin/kind

          # Verify installation
          kind version

          # Configure Kind to use Podman explicitly
          echo "Podman driver will be used by Kind"

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/kubectl
          kubectl version --client

      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          helm version

      - name: Set up environment and Podman configuration
        run: |
          echo "KIND_CLUSTER_NAME=ros-ocp-test-cluster" >> $GITHUB_ENV
          echo "HELM_RELEASE_NAME=ros-ocp-test" >> $GITHUB_ENV
          echo "NAMESPACE=ros-ocp-test" >> $GITHUB_ENV

      - name: Setup KIND cluster
        timeout-minutes: 15
        run: |
          cd scripts
          export KIND_CLUSTER_NAME=${{ env.KIND_CLUSTER_NAME }}
          export HELM_RELEASE_NAME=${{ env.HELM_RELEASE_NAME }}
          export NAMESPACE=${{ env.NAMESPACE }}

          # Make scripts executable
          chmod +x deploy-kind.sh

          # Setup KIND cluster
          echo "Setting up KIND cluster..."
          ./deploy-kind.sh

          # Verify cluster is running
          echo "Verifying cluster status..."
          kind get clusters
          kubectl cluster-info
          kubectl get nodes -o wide


      - name: Deploy Kafka Infrastructure
        timeout-minutes: 10
        run: |
          cd scripts

          # Make script executable
          chmod +x deploy-strimzi.sh

          # Deploy Strimzi and Kafka (uses default KAFKA_NAMESPACE=kafka)
          echo "Deploying Kafka infrastructure..."
          ./deploy-strimzi.sh

          # Verify Kafka deployment
          echo "Verifying Kafka deployment..."
          kubectl get pods -n kafka
          kubectl get kafka -n kafka

      - name: Deploy Helm Chart
        timeout-minutes: 20
        run: |
          # Use local Helm chart instead of pulling from GitHub
          cd scripts
          export KIND_CLUSTER_NAME=${{ env.KIND_CLUSTER_NAME }}
          export HELM_RELEASE_NAME=${{ env.HELM_RELEASE_NAME }}
          export NAMESPACE=${{ env.NAMESPACE }}
          export USE_LOCAL_CHART=true
          export LOCAL_CHART_PATH=../ros-ocp
          export HELM_TIMEOUT=900s

          # Make scripts executable
          chmod +x install-helm-chart.sh

          # Deploy Helm chart using local chart
          echo "Installing ROS-OCP Helm chart from local source..."
          echo "Using local chart: $LOCAL_CHART_PATH"
          echo "Using Helm timeout: $HELM_TIMEOUT"
          ./install-helm-chart.sh

          # Verify deployment
          echo "Verifying Helm chart deployment..."
          kubectl get pods -n $NAMESPACE
          kubectl get services -n $NAMESPACE
          kubectl get ingress -n $NAMESPACE

      - name: Wait for services to stabilize and verify authentication
        run: |
          echo "Waiting for services to stabilize..."
          sleep 60

          # Kubernetes authentication verification (1.24+ compatible)
          echo "=== Kubernetes Authentication Setup ==="
          
          # Check if service account exists
          if kubectl get serviceaccount insights-ros-ingress -n ${{ env.NAMESPACE }} >/dev/null 2>&1; then
            echo "✅ insights-ros-ingress service account found"
            
            # Create service account token using modern TokenRequest API (Kubernetes 1.24+)
            echo "Creating service account token using TokenRequest API..."
            if TOKEN=$(kubectl create token insights-ros-ingress -n ${{ env.NAMESPACE }} --duration=3600s 2>/dev/null); then
              if [ -n "$TOKEN" ] && [ ${#TOKEN} -gt 50 ]; then
                echo "✅ Service account token created successfully (length: ${#TOKEN})"
                echo "✅ Modern Kubernetes authentication is working"
                
                # Test token validation
                if kubectl auth can-i get pods --as=system:serviceaccount:${{ env.NAMESPACE }}:insights-ros-ingress -n ${{ env.NAMESPACE }} >/dev/null 2>&1; then
                  echo "✅ Service account has proper RBAC permissions"
                else
                  echo "⚠️  Service account may have limited RBAC permissions (this is normal for testing)"
                fi
              else
                echo "❌ Service account token is invalid or too short"
                exit 1
              fi
            else
              echo "❌ Failed to create service account token"
              echo "This may indicate RBAC or API server issues"
              exit 1
            fi
          else
            echo "❌ insights-ros-ingress service account not found"
            echo "Available service accounts:"
            kubectl get serviceaccounts -n ${{ env.NAMESPACE }} || true
            exit 1
          fi

          # Legacy authentication check (for reference, but not required)
          echo "=== Legacy Authentication Check (for reference) ==="
          if [ -f "/tmp/dev-kubeconfig" ]; then
            echo "ℹ️  Legacy kubeconfig found at /tmp/dev-kubeconfig"
          else
            echo "ℹ️  Legacy kubeconfig not found (this is expected with modern setup)"
          fi
          
          # Legacy token secret check (not required in Kubernetes 1.24+)
          if kubectl get secret insights-ros-ingress-token -n ${{ env.NAMESPACE }} >/dev/null 2>&1; then
            echo "ℹ️  Legacy token secret found (unusual in Kubernetes 1.24+)"
          else
            echo "ℹ️  Legacy token secret not found (this is expected in Kubernetes 1.24+)"
          fi

          echo "=== Pod Readiness Check ==="
          # Wait for all pods to be in Running state
          timeout 600 bash -c "until kubectl wait --for=condition=ready pod -l 'app.kubernetes.io/instance=${{ env.HELM_RELEASE_NAME }}' -n ${{ env.NAMESPACE }} --timeout=30s; do echo 'Waiting for pods...'; sleep 10; done"

          # Additional wait for services to fully initialize
          echo "All pods ready, waiting additional 30 seconds for service initialization..."
          sleep 30

      - name: Show cluster status on failure
        if: failure()
        run: |
          echo "=== Container Runtime Status ==="
          ${{ env.CONTAINER_RUNTIME }} --version || true
          ${{ env.CONTAINER_RUNTIME }} info || true
          ${{ env.CONTAINER_RUNTIME }} ps -a || true
          ${{ env.CONTAINER_RUNTIME }} images | head -10 || true

          echo "=== Kind Status ==="
          kind version || true
          kind get clusters || true

          echo "=== Cluster Status ==="
          kubectl cluster-info || true
          kubectl version || true

          echo "=== Kafka Infrastructure Status ==="
          echo "Kafka Pods:"
          kubectl get pods -n kafka -o wide || true
          echo "Kafka Cluster:"
          kubectl get kafka -n kafka || true
          echo "Kafka Topics:"
          kubectl get kafkatopic -n kafka || true
          echo "Strimzi Operator:"
          kubectl get pods -n kafka -l name=strimzi-cluster-operator || true
          echo "Kafka Events:"
          kubectl get events -n kafka --sort-by='.lastTimestamp' || true

          echo "=== ROS-OCP Pods Status ==="
          kubectl get pods -n ${{ env.NAMESPACE }} -o wide || true
          kubectl get pods --all-namespaces | head -20 || true

          echo "=== Services Status ==="
          kubectl get services -n ${{ env.NAMESPACE }} || true

          echo "=== Events ==="
          kubectl get events -n ${{ env.NAMESPACE }} --sort-by='.lastTimestamp' || true
          kubectl get events --all-namespaces --sort-by='.lastTimestamp' | head -20 || true

          echo "=== Persistent Volumes ==="
          kubectl get pv,pvc -n ${{ env.NAMESPACE }} || true
          kubectl get pv,pvc -n kafka || true

          echo "=== Node Status ==="
          kubectl get nodes -o wide || true
          kubectl describe nodes || true

          echo "=== Authentication Status ==="
          echo "Service Accounts:"
          kubectl get serviceaccounts -n ${{ env.NAMESPACE }} || true
          echo "Service Account Details:"
          kubectl describe serviceaccount insights-ros-ingress -n ${{ env.NAMESPACE }} || true
          echo "Secrets:"
          kubectl get secrets -n ${{ env.NAMESPACE }} | grep -E "(insights-ros-ingress|token)" || true
          echo "All Secrets:"
          kubectl get secrets -n ${{ env.NAMESPACE }} || true
          echo "Authentication files:"
          ls -la /tmp/ | grep -E "(kubeconfig|auth)" || true
          echo "Testing TokenRequest API:"
          kubectl create token insights-ros-ingress -n ${{ env.NAMESPACE }} --duration=60s --dry-run=server || true
          echo "RBAC Check:"
          kubectl auth can-i get pods --as=system:serviceaccount:${{ env.NAMESPACE }}:insights-ros-ingress -n ${{ env.NAMESPACE }} || true

          echo "=== Kafka/Strimzi Logs ==="
          echo "Strimzi Operator Logs:"
          kubectl logs -n kafka -l name=strimzi-cluster-operator --tail=50 || true
          echo "Kafka Broker Logs:"
          kubectl logs -n kafka -l strimzi.io/name=ros-ocp-kafka-kafka --tail=30 || true
          
          echo "=== Ingress Service Logs ==="
          kubectl logs -n ${{ env.NAMESPACE }} -l app.kubernetes.io/name=ingress --tail=30 || true
          echo "=== Recent Logs ==="
          for pod in $(kubectl get pods -n ${{ env.NAMESPACE }} -o name | head -5); do
            echo "--- Logs for $pod ---"
            kubectl logs -n ${{ env.NAMESPACE }} $pod --tail=20 || true
            kubectl logs -n ${{ env.NAMESPACE }} $pod --previous --tail=10 || true
          done

          echo "=== Container Logs ==="
          for container in $(${{ env.CONTAINER_RUNTIME }} ps --filter "label=io.x-k8s.kind.cluster=${{ env.KIND_CLUSTER_NAME }}" --format "{{.Names}}" | head -3); do
            echo "--- ${{ env.CONTAINER_RUNTIME }} logs for $container ---"
            ${{ env.CONTAINER_RUNTIME }} logs $container --tail=20 || true
          done

          echo "=== System Resources ==="
          df -h || true
          free -h || true
          top -bn1 | head -20 || true

      - name: Cleanup
        if: always()
        run: |
          kind delete cluster --name ${{ env.KIND_CLUSTER_NAME }} || true
